{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK57OgNKkSVjEwaYg3NVrG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikitaj832/Training/blob/main/Introduction_to_Apache_Spark%2C_Spark_History.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "GGOatu3616Fb",
        "outputId": "6afbc8fc-c160-4f18-a34e-dcfc6d3cbe9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [\u001b[0m\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\u001b[33m\r0% [2 InRelease 15.6 kB/128 kB 12%] [Waiting for headers] [Connected to cloud.r\u001b[0m\r                                                                               \rGet:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "\u001b[33m\r0% [2 InRelease 59.1 kB/128 kB 46%] [Waiting for headers] [Waiting for headers]\u001b[0m\u001b[33m\r0% [2 InRelease 62.0 kB/128 kB 48%] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                               \rGet:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,741 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,003 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,295 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,739 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,630 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,553 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,979 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.8 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,468 kB]\n",
            "Fetched 32.1 MB in 9s (3,444 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f311c63c5d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2a8d9200eec5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# install pyspark\n",
        "\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Apache Spark?\n",
        "\n",
        "Apache Spark is a free and open-source platform designed for distributed data processing, ideal for handling large-scale data workloads. It leverages in-memory computation and optimized query execution to deliver high-speed analytical processing on datasets of all sizes. Spark offers programming interfaces in Java, Scala, Python, and R, allowing developers to write reusable code across diverse tasks such as batch processing, real-time data streaming, interactive querying, machine learning, and graph computations. It is widely adopted across various sectors, with users including organizations like FINRA, Yelp, Zillow, DataXu, Urban Institute, and CrowdStrike.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YKHnbdpN5OLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What problems does Spark solve?\n",
        "\n",
        "1. **Slow Data Processing**-\n",
        "Traditional systems like Hadoop MapReduce are slower because they use disk storage for processing. Spark uses in-memory computing, which makes data processing much faster.\n",
        "\n",
        "2. **Need for Multiple Tools**-\n",
        "Earlier, different tools were required for different tasksâ€”Hadoop for batch jobs, Storm for real-time, Mahout for machine learning. Spark combines all these in one platform.\n",
        "\n",
        "3. **Scalability Issues** -\n",
        "Many systems can't handle large-scale data efficiently. Spark is built to work on clusters of thousands of machines and can handle very large datasets.\n",
        "\n",
        "4. **Limited Language Options** -\n",
        "Older tools often support only one or two programming languages. Spark supports Java, Scala, Python, and R, giving developers more flexibility.\n",
        "\n",
        "5. **Hard to Debug and Monitor**-\n",
        "Debugging and tracking performance in big data systems can be difficult. Spark offers tools like a web-based user interface to monitor and manage jobs easily.\n",
        "\n",
        "6. **No Real-Time Processing** -\n",
        "Traditional batch systems cannot process live data. Spark supports real-time data processing using Structured Streaming.\n",
        "\n",
        "7. **Poor Data Source Integration** -\n",
        "Some systems struggle to connect with different data sources. Spark easily connects with HDFS, Hive, Cassandra, HBase, Amazon S3, and many others.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLioTAov6DN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are the key features of Spark?\n",
        "\n",
        "1. **Speed**-\n",
        "Spark processes data very fast using in-memory\n",
        "\n",
        "*   Spark processes data very fast using in-memory computing.\n",
        "*   It is much faster than traditional tools like Hadoop MapReduce.\n",
        "\n",
        "2. **Ease of Use**-\n",
        "* Spark provides simple APIs in Java, Scala, Python, and R.\n",
        "* It supports SQL, DataFrames, and RDDs (Resilient Distributed Datasets), making it easy to work with different data types.\n",
        "\n",
        "3. **Unified Engine**-\n",
        "* Spark can handle different types of data processing:\n",
        "\n",
        "* Batch processing\n",
        "\n",
        "* Real-time streaming\n",
        "\n",
        "* Machine learning (MLlib)\n",
        "\n",
        "* Graph processing (GraphX)\n",
        "\n",
        "* SQL queries (Spark SQL)\n",
        "\n",
        "4. **Advanced Analytics**-\n",
        "* Spark supports complex operations like:\n",
        "\n",
        "* Data mining\n",
        "\n",
        "* Machine learning\n",
        "\n",
        "* Graph algorithms\n",
        "\n",
        "5. **Real-Time Processing**-\n",
        "With Structured Streaming, Spark can process live data as it arrives.\n",
        "\n",
        "6. **Fault Tolerance**-\n",
        "Spark automatically recovers from failures using RDDs, which keep track of data transformations.\n",
        "\n",
        "7. **Scalability**-\n",
        "Spark can run on a single laptop or on thousands of machines in a cluster, processing petabytes of data.\n",
        "\n",
        "8. **Support for Multiple Data Sources**-\n",
        "Spark can connect to many data sources like:\n",
        "\n",
        "* HDFS\n",
        "\n",
        "* Hive\n",
        "\n",
        "* Cassandra\n",
        "\n",
        "* HBase\n",
        "\n",
        "* Amazon S3\n",
        "\n",
        "* JDBC-compatible databases\n",
        "\n",
        "9. **Active Community and Open Source** -\n",
        "Spark is open-source and has a large community, so updates and support are widely available.\n",
        "\n"
      ],
      "metadata": {
        "id": "GFXtZiab8ZXV"
      }
    }
  ]
}