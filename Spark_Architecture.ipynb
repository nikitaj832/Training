{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFc6BL7On/SVeAENFy8Dp8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikitaj832/Training/blob/main/Spark_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Spark architecture overview.**\n",
        "Apache Spark follows a master-slave architecture consisting mainly of the following components:\n",
        "\n",
        "1. **Driver Program**\n",
        "* The Driver is the central coordinator of the Spark application.\n",
        "\n",
        "* It runs the main() function of your application.\n",
        "\n",
        "* It creates the SparkContext, which connects to the cluster manager.\n",
        "\n",
        "* It breaks down the application into tasks, builds a DAG (Directed Acyclic Graph) of stages, and schedules tasks on executors.\n",
        "\n",
        "* Tracks the overall progress and handles failures.\n",
        "\n",
        "2. **Cluster Manager**\n",
        "* Responsible for resource management in the cluster.\n",
        "\n",
        "* Examples: Standalone cluster manager, YARN, Mesos, or Kubernetes.\n",
        "\n",
        "* Allocates resources (CPU, memory) to Spark applications.\n",
        "\n",
        "3. **Worker Nodes**\n",
        "* These are the nodes in the cluster where the actual computation happens.\n",
        "\n",
        "* Each worker node runs one or more Executors.\n",
        "\n",
        "4. **Executors**\n",
        "* Executors are processes launched on worker nodes.\n",
        "\n",
        "* They execute the tasks assigned by the Driver.\n",
        "\n",
        "* Responsible for running computations and storing data in memory or disk.\n",
        "\n",
        "* Send results back to the Driver.\n"
      ],
      "metadata": {
        "id": "cyh3zTqEKWjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1 What role does the Driver play in Spark?**\n",
        "In Apache Spark, the Driver plays a crucial role as the main control process of a Spark application. Here’s what the Driver does:\n",
        "\n",
        "* Coordinates the execution of the Spark application.\n",
        "\n",
        "* Creates the SparkContext, which is the entry point to interact with the Spark cluster.\n",
        "\n",
        "* Transforms user code into tasks and schedules these tasks to run on Executors.\n",
        "\n",
        "* Maintains metadata about the job, like the DAG (Directed Acyclic Graph) of stages and tasks.\n",
        "\n",
        "* Monitors the status of the tasks running on the cluster and handles failures.\n",
        "\n",
        "* Collects and aggregates results from Executors."
      ],
      "metadata": {
        "id": "4POTvb89LxH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. What are Executors responsible for?**\n",
        "\n",
        "1. Executing Tasks\n",
        "* Executors run the tasks (units of work) assigned by the Driver.\n",
        "* Tasks are the actual code (like transformations and actions) that needs to run on the data.\n",
        "\n",
        "2. Data Storage\n",
        "* Executors store intermediate data (for shuffles) and cached data (for reuse).\n",
        "* They use memory and disk for storing RDDs or DataFrames when caching/persisting is used.\n",
        "3. Reporting Back to the Driver\n",
        "* Executors send task status updates, progress information, and results back to the Driver.\n",
        "* They report failures, which the Driver can use to reschedule tasks.\n",
        "4. Managing Their Own Resources\n",
        "* Each executor has its own memory and CPU.\n",
        "* They are long-lived processes and are launched once per Spark application.\n"
      ],
      "metadata": {
        "id": "T3rT5QWXxM31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **What is Local mode and Cluster mode in Spark?**\n",
        "\n",
        "1. Local Mode\n",
        "* Everything runs on a single machine — the Driver and the Executors.\n",
        "\n",
        "     Useful for:\n",
        "* Development\n",
        "* Testing\n",
        "* Debugging\n",
        "\n",
        "\n",
        "2. Cluster Mode\n",
        "* The Driver runs on one of the cluster nodes, and Executors run on other worker nodes.\n",
        "* Used for production and large-scale Spark applications.\n",
        "\n",
        "  Requires a cluster manager, such as:\n",
        "* YARN\n",
        "* Mesos\n",
        "* Kubernetes\n",
        "* Standalone"
      ],
      "metadata": {
        "id": "6E-M19QSy_Az"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLngJjQHzsbj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}