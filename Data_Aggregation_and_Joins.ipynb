{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMug8jXKU2dii53ahw47I9a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikitaj832/Training/blob/main/Data_Aggregation_and_Joins.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Aggregation and Joins**"
      ],
      "metadata": {
        "id": "YVEIr5J5Ca7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Group data by a column and calculate average, max, and sum**"
      ],
      "metadata": {
        "id": "cFRr2cMRDm3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKemDg07CV_a",
        "outputId": "9816bcca-a932-469a-c906-5859db6dd000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+---------+-----------+\n",
            "|   Category|Average_Sales|Max_Sales|Total_Sales|\n",
            "+-----------+-------------+---------+-----------+\n",
            "|Electronics|      36000.0|    70000|      72000|\n",
            "|   Footwear|       2250.0|     3000|       4500|\n",
            "|   Clothing|       1700.0|     2200|       3400|\n",
            "+-----------+-------------+---------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg, max, sum\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"GroupByProduct\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (\"Laptop\", \"Electronics\", 70000),\n",
        "    (\"Headphones\", \"Electronics\", 2000),\n",
        "    (\"Shoes\", \"Footwear\", 3000),\n",
        "    (\"Sandals\", \"Footwear\", 1500),\n",
        "    (\"T-Shirt\", \"Clothing\", 1200),\n",
        "    (\"Jeans\", \"Clothing\", 2200)\n",
        "]\n",
        "\n",
        "columns = [\"Product\", \"Category\", \"Sales\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Group by Category and calculate average, max, and total sales\n",
        "result_df = df.groupBy(\"Category\").agg(\n",
        "    avg(\"Sales\").alias(\"Average_Sales\"),\n",
        "    max(\"Sales\").alias(\"Max_Sales\"),\n",
        "    sum(\"Sales\").alias(\"Total_Sales\")\n",
        ")\n",
        "\n",
        "# Show result\n",
        "result_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Sort employees by salary in ascending and descending order**"
      ],
      "metadata": {
        "id": "Y-3_9CnzEkxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"SortEmployees\").getOrCreate()\n",
        "\n",
        "# Sample employee data\n",
        "data = [\n",
        "    (\"Nikita\", \"HR\", 50000),\n",
        "    (\"Rahul\", \"IT\", 65000),\n",
        "    (\"Anjali\", \"Finance\", 48000),\n",
        "    (\"Ravi\", \"IT\", 70000),\n",
        "    (\"Priya\", \"HR\", 52000)\n",
        "]\n",
        "\n",
        "columns = [\"Name\", \"Department\", \"Salary\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# 1. Sort by Salary in Ascending Order\n",
        "print(\"Ascending Order:\")\n",
        "df.orderBy(\"Salary\").show()\n",
        "\n",
        "# 2. Sort by Salary in Descending Order\n",
        "print(\"Descending Order:\")\n",
        "df.orderBy(df.Salary.desc()).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4WOoBBKEBDD",
        "outputId": "f467f12e-5246-47d6-f2a8-748f58e3de2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ascending Order:\n",
            "+------+----------+------+\n",
            "|  Name|Department|Salary|\n",
            "+------+----------+------+\n",
            "|Anjali|   Finance| 48000|\n",
            "|Nikita|        HR| 50000|\n",
            "| Priya|        HR| 52000|\n",
            "| Rahul|        IT| 65000|\n",
            "|  Ravi|        IT| 70000|\n",
            "+------+----------+------+\n",
            "\n",
            "Descending Order:\n",
            "+------+----------+------+\n",
            "|  Name|Department|Salary|\n",
            "+------+----------+------+\n",
            "|  Ravi|        IT| 70000|\n",
            "| Rahul|        IT| 65000|\n",
            "| Priya|        HR| 52000|\n",
            "|Nikita|        HR| 50000|\n",
            "|Anjali|   Finance| 48000|\n",
            "+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Perform an inner join between two DataFrames using \"customer_id\"  and use diff exmple**"
      ],
      "metadata": {
        "id": "NaQSWjjiFlKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"InnerJoinExample\").getOrCreate()\n",
        "\n",
        "# Customer DataFrame\n",
        "customer_data = [\n",
        "    (101, \"Nikita\"),\n",
        "    (102, \"Rahul\"),\n",
        "    (103, \"Anjali\"),\n",
        "    (104, \"Ravi\")\n",
        "]\n",
        "customer_columns = [\"customer_id\", \"customer_name\"]\n",
        "df_customers = spark.createDataFrame(customer_data, customer_columns)\n",
        "\n",
        "# Orders DataFrame\n",
        "order_data = [\n",
        "    (201, 101, \"Laptop\"),\n",
        "    (202, 102, \"Mobile\"),\n",
        "    (203, 105, \"Headphones\"),  # 105 does not exist in customers\n",
        "    (204, 103, \"Keyboard\")\n",
        "]\n",
        "order_columns = [\"order_id\", \"customer_id\", \"product\"]\n",
        "df_orders = spark.createDataFrame(order_data, order_columns)\n",
        "\n",
        "# Perform INNER JOIN on 'customer_id'\n",
        "joined_df = df_customers.join(df_orders, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Show the result\n",
        "joined_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8ZAYzJlExvG",
        "outputId": "7f7764ad-b3ed-4e0c-e8d4-fe03ab961b58"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------+--------+\n",
            "|customer_id|customer_name|order_id| product|\n",
            "+-----------+-------------+--------+--------+\n",
            "|        101|       Nikita|     201|  Laptop|\n",
            "|        102|        Rahul|     202|  Mobile|\n",
            "|        103|       Anjali|     204|Keyboard|\n",
            "+-----------+-------------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b_rURru9FRGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aiTxteXBFw41"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}